{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Question 1: </font>\n",
    "## <font color=\"sky blue\">Create one array of actual values and another array of predicted values. Compare the two sets with the confusion matrix.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[4 1]\n",
      " [2 3]]\n",
      "\n",
      "Outcome values : \n",
      " 4 1 2 3\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.80      0.73         5\n",
      "           0       0.75      0.60      0.67         5\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.71      0.70      0.70        10\n",
      "weighted avg       0.71      0.70      0.70        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix in sklearn\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# actual values\n",
    "actual =    [1,0,0,1,0,0,1,0,1,1]\n",
    "# predicted values\n",
    "predicted = [1,0,0,1,1,0,0,1,1,1]\n",
    "\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('\\nOutcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"green\">Question 2: </font>\n",
    "## <font color=\"chartreuse\">Find out the recall, precision, F1 score and confusion matrix with picture</font>\n",
    "<img src=\"cat-dog.png\" height=200 width=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall/Sensitivity = 0.84\n",
      "Precision = 0.74\n",
      "Specificity = 0.64\n",
      "F1 Score = 0.7868354430379746\n"
     ]
    }
   ],
   "source": [
    "\"\"\" The confusion matrix is of the form :-\n",
    "\n",
    "    TP  FN\n",
    "    \n",
    "    FP  TN         \"\"\"\n",
    "\n",
    "Recall = 42/(42+8)                # TP/(TP+FN)\n",
    "Precision = (42+32)/(42+8+18+32)  # (TP+TN)/(TP+FN+FP+TN)\n",
    "Specificity = 32/(18+32)          # TN/(FP+TN)\n",
    "F_Score = 2*(Recall*Precision)/(Recall+Precision)\n",
    "\n",
    "print(\"Recall/Sensitivity =\",Recall)\n",
    "print(\"Precision =\",Precision)\n",
    "print(\"Specificity =\",Specificity)\n",
    "print(\"F1 Score =\",F_Score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
